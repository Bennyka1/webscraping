---
title: "Scrapingbee - Benjamin Kampka - 35017"
output: html_notebook
---

# Parsing a webpage using R

```{r}
library(tidyverse)
library(RCurl)
scrape_url <- "https://www.scrapingbee.com/"
flat_html <- readLines(scrape_url)
head(flat_html,n=20)
```

### Scrape Google

```{r}
scrape_url2 <- "https://www.google.de"
flat_html2 <- readLines(scrape_url2)
flat_html2
```

### Scrape a Simple Webpage

```{r}
scrape_url3 <- "https://www.york.ac.uk/teaching/cws/wws/webpage1.html"
flat_html3 <- readLines(scrape_url3)
flat_html3
```

### ftp access 

```{r}
ftp_url <- "ftp://cran.r-project.org/pub/R/web/packages/BayesMixSurv/"
get_files <- getURL(ftp_url, dirlistonly = TRUE)
get_files
```


Filenames are: 
```{r}
all_filenames <- str_split(get_files,"\n")
all_filenames
```


```{r}
extracted_filenames <- str_split(get_files, "\r\n")[[1]]
extracted_html_filenames <-unlist(str_extract_all(extracted_filenames, ".+(.html)"))
extracted_html_filenames
```

Download HTML Docs:

```{r}
FTPDownloader <- function(filename, folder, handle) {

  dir.create(folder, showWarnings = FALSE)

  fileurl <- str_c(ftp_url, filename)

  if (!file.exists(str_c(folder, "/", filename))) {

    file_name <- try(getURL(fileurl, curl = handle))

    write(file_name, str_c(folder, "/", filename))

    Sys.sleep(1)

  }

} 

```

```{r}
Curlhandle <- getCurlHandle(ftp.use.epsv = FALSE)
```

```{r}
library("plyr")
l_ply(extracted_html_filenames, FTPDownloader,folder = "scrapingbee_html", handle = Curlhandle)
list.files("./scrapingbee_html")
```

### Scraping information from Wikipedia using R

```{r}
wiki_url <- "https://en.wikipedia.org/wiki/Leonardo_da_Vinci"
wiki_read <- readLines(wiki_url, encoding = "UTF-8")
library("XML")
parsed_wiki <- htmlParse(wiki_read, encoding = "UTF-8")
wiki_intro_text <- parsed_wiki["//p"]
wiki_intro_text[[4]]
```

```{r}
getHTMLLinks(wiki_read)
```

```{r}
length(getHTMLLinks(wiki_read))
```

```{r}
wiki_url1 <- "https://en.wikipedia.org/wiki/Help:Table"
wiki_read1 <- readLines(wiki_url1, encoding = "UTF-8")
length((readHTMLTable(wiki_read1)))
```

```{r}
names(readHTMLTable(wiki_read1))
```

```{r}
readHTMLTable(wiki_read1)$"The table's caption\n"
```

```{r}
wiki_url2 <- "https://en.wikipedia.org/wiki/United_States_Census"
wiki_read2 <- readLines(wiki_url2, encoding = "UTF-8")
length((readHTMLTable(wiki_read2)))
```

```{r}
names(readHTMLTable(wiki_read2))

```

```{r}
readHTMLTable(wiki_read2)[3]
```

### Handling HTML forms while scraping with R



Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

